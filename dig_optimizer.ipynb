{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell loads the model from the config file and initializes the viewer\n",
    "'''\n",
    "# %matplotlib widget\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from nerfstudio.viewer.viewer import Viewer\n",
    "from nerfstudio.configs.base_config import ViewerConfig\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from typing import List,Optional,Literal\n",
    "from nerfstudio.utils import writer\n",
    "import time\n",
    "from threading import Lock\n",
    "\n",
    "# config = Path(\"outputs/nerfgun2/dig/2024-05-03_161203/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun3/dig/2024-05-03_170424/config.yml\")\n",
    "config = Path(\"outputs/nerfgun4/dig/2024-05-07_130351/config.yml\")\n",
    "# config = Path(\"outputs/painter_sculpture/dig/2024-05-10_132522/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-09_123412/config.yml\")\n",
    "# config = Path(\"outputs/bww_faucet/dig/2024-05-07_141805/config.yml\")\n",
    "# config = Path(\"outputs/boops_mug/dig/2024-05-10_223745/config.yml\")\n",
    "train_config,pipeline,_,_ = eval_setup(config)\n",
    "dino_loader = pipeline.datamanager.dino_dataloader\n",
    "train_config.logging.local_writer.enable = False\n",
    "# We need to set up the writer to track number of rays, otherwise the viewer will not calculate the resolution correctly\n",
    "writer.setup_local_writer(train_config.logging, max_iter=train_config.max_num_iterations)\n",
    "v = Viewer(ViewerConfig(default_composite_depth=False,num_rays_per_chunk=-1),config.parent,pipeline.datamanager.get_datapath(),pipeline,train_lock=Lock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "from typing import Union\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model.to('cuda')\n",
    "def get_depth(img: Union[torch.tensor,np.ndarray]):\n",
    "    assert img.shape[2] == 3\n",
    "    if isinstance(img,torch.Tensor):\n",
    "        img = img.cpu().numpy()\n",
    "    image = Image.fromarray(img)\n",
    "\n",
    "    # prepare image for the model\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    # interpolate to original size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    return prediction.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell defines a simple pose optimizer for learning a rigid transform offset given a gaussian model, star pose, and starting view\n",
    "\"\"\"\n",
    "from torch import Tensor\n",
    "from lerf.dig import DiGModel\n",
    "from lerf.data.utils.dino_dataloader import DinoDataloader\n",
    "from nerfstudio.cameras.cameras import Cameras\n",
    "from copy import deepcopy\n",
    "from torchvision.transforms.functional import resize\n",
    "import torchvision\n",
    "from gsplat._torch_impl import quat_to_rotmat,normalized_quat_to_rotmat\n",
    "from contextlib import nullcontext\n",
    "from lerf.zed import Zed\n",
    "from nerfstudio.engine.schedulers import ExponentialDecayScheduler,ExponentialDecaySchedulerConfig\n",
    "\n",
    "def get_vid_frame(cap,timestamp):\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate the frame number based on the timestamp and fps\n",
    "    frame_number = min(int(timestamp * fps),int(cap.get(cv2.CAP_PROP_FRAME_COUNT)-1))\n",
    "    \n",
    "    # Set the video position to the calculated frame number\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    \n",
    "    # Read the frame\n",
    "    success, frame = cap.read()\n",
    "    # convert BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return frame\n",
    "        \n",
    "def quatmul(q0:torch.Tensor,q1:torch.Tensor):\n",
    "    w0, x0, y0, z0 = torch.unbind(q0, dim=-1)\n",
    "    w1, x1, y1, z1 = torch.unbind(q1, dim=-1)\n",
    "    return torch.stack(\n",
    "            [\n",
    "                -x0 * x1 - y0 * y1 - z0 * z1 + w0 * w1,\n",
    "                x0 * w1 + y0 * z1 - z0 * y1 + w0 * x1,\n",
    "                -x0 * z1 + y0 * w1 + z0 * x1 + w0 * y1,\n",
    "                x0 * y1 - y0 * x1 + z0 * w1 + w0 * z1,\n",
    "            ],\n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "def depth_ranking_loss(rendered_depth, gt_depth):\n",
    "    \"\"\"\n",
    "    Depth ranking loss as described in the SparseNeRF paper\n",
    "    Assumes that the layout of the batch comes from a PairPixelSampler, so that adjacent samples in the gt_depth\n",
    "    and rendered_depth are from pixels with a radius of each other\n",
    "    \"\"\"\n",
    "    m = 1e-4\n",
    "    if rendered_depth.shape[0] % 2 != 0:\n",
    "        # chop off one index\n",
    "        rendered_depth = rendered_depth[:-1, :]\n",
    "        gt_depth = gt_depth[:-1, :]\n",
    "    dpt_diff = gt_depth[::2, :] - gt_depth[1::2, :]\n",
    "    out_diff = rendered_depth[::2, :] - rendered_depth[1::2, :] + m\n",
    "    differing_signs = torch.sign(dpt_diff) != torch.sign(out_diff)\n",
    "    loss = (out_diff[differing_signs] * torch.sign(out_diff[differing_signs]))\n",
    "    med = loss.median()\n",
    "    return loss[loss < med].mean()\n",
    "\n",
    "class ATAPLoss:\n",
    "    touch_radius: float = .01\n",
    "    N: int = 200\n",
    "    loss_mult: float = .05\n",
    "    def __init__(self, dig_model: DiGModel, group_masks: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Initializes the data structure to compute the loss between groups touching\n",
    "        \"\"\"\n",
    "        self.dig_model = dig_model\n",
    "        self.group_masks = group_masks\n",
    "        self.nn_info = []\n",
    "        for grp in self.group_masks:\n",
    "            with torch.no_grad():\n",
    "                dists, ids, match_ids, group_ids = self._radius_nn(grp, self.touch_radius)\n",
    "                self.nn_info.append((dists, ids, match_ids, group_ids))\n",
    "                print(f\"Group {len(self.nn_info)} has {len(ids)} neighbors\")\n",
    "        \n",
    "\n",
    "    def __call__(self, connectivity_weights: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Computes the loss between groups touching\n",
    "        connectivity_weights: a tensor of shape (num_groups,num_groups) representing the weights between each group\n",
    "\n",
    "        returns: a differentiable loss\n",
    "        \"\"\"\n",
    "        assert connectivity_weights.shape == (len(self.group_masks),len(self.group_masks)), \"connectivity weights must be a square matrix of size num_groups\"\n",
    "        atap_loss = torch.zeros(1,dtype=torch.float32,device='cuda')\n",
    "        for i,grp in enumerate(self.group_masks):\n",
    "            orig_dists, ids, match_ids, group_ids = self.nn_info[i]\n",
    "            match_means = self.dig_model.means[match_ids]\n",
    "            grp_means = self.dig_model.means[ids]\n",
    "            dists = (grp_means - match_means).norm(dim=-1)\n",
    "            # weight the dists by the connectivity weights\n",
    "            weights_for_this_group = connectivity_weights[i]\n",
    "            connection_weights = weights_for_this_group[group_ids]\n",
    "            dists_loss_prereduce = (dists - orig_dists).abs() * connection_weights * self.loss_mult\n",
    "            dists_loss = dists_loss_prereduce.mean()\n",
    "            if not dists_loss.isnan():\n",
    "                atap_loss = atap_loss + dists_loss\n",
    "            else:\n",
    "                pass\n",
    "        return atap_loss\n",
    "        \n",
    "\n",
    "    def _radius_nn(self, group_mask: torch.Tensor, r: float):\n",
    "        \"\"\"\n",
    "        returns the nearest neighbors to gaussians in a group within a certain radius (and outside that group)\n",
    "        returns -1 indices for neighbors outside the radius or within the same group\n",
    "        \"\"\"\n",
    "        global_group_ids = torch.zeros(self.dig_model.num_points,dtype=torch.long,device='cuda')\n",
    "        for i,grp in enumerate(self.group_masks):\n",
    "            global_group_ids[grp] = i\n",
    "        from cuml.neighbors import NearestNeighbors\n",
    "        model = NearestNeighbors(n_neighbors=self.N)\n",
    "        means = self.dig_model.means.detach().cpu().numpy()\n",
    "        model.fit(means)\n",
    "        dists, match_ids = model.kneighbors(means)\n",
    "        dists, match_ids = torch.tensor(dists,dtype=torch.float32,device='cuda'),torch.tensor(match_ids,dtype=torch.long,device='cuda')\n",
    "        dists, match_ids = dists[group_mask], match_ids[group_mask]\n",
    "        # filter matches outside the radius\n",
    "        match_ids[dists>r] = -1\n",
    "        # filter out ones within same group mask\n",
    "        match_ids[group_mask[match_ids]] = -1\n",
    "        ids = torch.arange(self.dig_model.num_points,dtype=torch.long,device='cuda')[group_mask].unsqueeze(-1).repeat(1,self.N)\n",
    "        #flatten all the ids/dists/match_ids\n",
    "        ids = ids[match_ids!=-1].flatten()\n",
    "        dists = dists[match_ids!=-1].flatten()\n",
    "        match_ids = match_ids[match_ids!=-1].flatten()\n",
    "        return dists, ids, match_ids, global_group_ids[match_ids]\n",
    "\n",
    "try:\n",
    "    loss_plt.remove()\n",
    "except:\n",
    "    pass\n",
    "class RigidGroupOptimizer:\n",
    "    use_depth: bool = True\n",
    "    use_atap: bool = True\n",
    "    pose_lr: float = .005\n",
    "    pose_lr_final: float = .0005\n",
    "    def __init__(self, dig_model: DiGModel, dino_loader: DinoDataloader, init_c2o: Cameras, group_masks: List[torch.Tensor], render_lock = nullcontext()):\n",
    "        \"\"\"\n",
    "        This one takes in a list of gaussian ID masks to optimize local poses for\n",
    "        Each rigid group can be optimized independently, with no skeletal constraints\n",
    "        \"\"\"\n",
    "        self.dig_model = dig_model\n",
    "        #detach all the params to avoid retain_graph issue\n",
    "        self.dig_model.gauss_params['means'] = self.dig_model.gauss_params['means'].detach()\n",
    "        self.dig_model.gauss_params['quats'] = self.dig_model.gauss_params['quats'].detach()\n",
    "        self.dino_loader = dino_loader\n",
    "        self.group_masks = group_masks\n",
    "        self.init_c2o = deepcopy(init_c2o).to('cuda')\n",
    "        #store a 7-vec of trans, rotation for each group\n",
    "        self.pose_deltas = torch.zeros(len(group_masks),7,dtype=torch.float32,device='cuda')\n",
    "        self.pose_deltas[:,3:] = torch.tensor([1,0,0,0],dtype=torch.float32,device='cuda')\n",
    "        self.pose_deltas = torch.nn.Parameter(self.pose_deltas)\n",
    "        #NOT USED RN\n",
    "        self.connectivity_weights = torch.nn.Parameter(-torch.ones(len(group_masks),len(group_masks),dtype=torch.float32,device='cuda'))\n",
    "        self.optimizer = torch.optim.Adam([self.pose_deltas],lr=self.pose_lr)\n",
    "        self.init_means = dig_model.gauss_params['means'].detach().clone()\n",
    "        self.init_quats = dig_model.gauss_params['quats'].detach().clone()\n",
    "        self.blur = torchvision.transforms.GaussianBlur(kernel_size=[13,13]).cuda()\n",
    "        self.keyframes = []\n",
    "        # lock to prevent blocking the render thread if provided\n",
    "        self.render_lock = render_lock\n",
    "        self.atap = ATAPLoss(dig_model,group_masks)\n",
    "        self.centroids = torch.empty((self.dig_model.num_points,3),dtype=torch.float32,device='cuda',requires_grad=False)\n",
    "        for i,mask in enumerate(self.group_masks):\n",
    "            with torch.no_grad():\n",
    "                self.centroids[mask] = self.dig_model.gauss_params['means'][mask].mean(dim=0)\n",
    "\n",
    "    def vec2_to_R(self,vec2):\n",
    "        \"\"\"\n",
    "        vec2: 6, shape tensor\n",
    "        returns [3,3] rotation matrix\n",
    "        \"\"\"\n",
    "        v1 = vec2[:3]\n",
    "        v2 = vec2[3:]\n",
    "        v1 = v1/v1.norm()\n",
    "        v2 = v2 - v2.dot(v1)*v1\n",
    "        v2 = v2/v2.norm()\n",
    "        v3 = torch.linalg.cross(v1,v2)#z\n",
    "        return torch.stack([v1,v2,v3],dim=1)\n",
    "\n",
    "    def step(self, niter = 1, use_depth = True, use_rgb = False, accumulate_grad = False, metric_depth = False):\n",
    "        scheduler = ExponentialDecayScheduler(ExponentialDecaySchedulerConfig(lr_final = self.pose_lr_final, max_steps=niter)).get_scheduler(self.optimizer, self.pose_lr)\n",
    "        for i in range(niter):\n",
    "            # renormalize rotation representation\n",
    "            with torch.no_grad():\n",
    "                self.pose_deltas[:,3:] = self.pose_deltas[:,3:]/self.pose_deltas[:,3:].norm(dim=1,keepdim=True)\n",
    "            if not accumulate_grad:\n",
    "                self.optimizer.zero_grad()\n",
    "            with self.render_lock:\n",
    "                self.dig_model.eval()\n",
    "                self.apply_to_model(self.pose_deltas)\n",
    "                dig_outputs = self.dig_model.get_outputs(self.init_c2o)\n",
    "            if 'dino' not in dig_outputs:\n",
    "                self.reset_transforms()\n",
    "                raise RuntimeError(\"Lost tracking\")\n",
    "            loss = 0\n",
    "            dino_feats = self.blur(dig_outputs[\"dino\"].permute(2,0,1)).permute(1,2,0)\n",
    "            pix_loss = (self.frame_pca_feats - dino_feats)\n",
    "            # THIS IS BAD WE NEED TO FIX THIS (because resizing makes the image very slightly misaligned)\n",
    "            loss = pix_loss.norm(dim=-1).mean()\n",
    "            if use_depth and self.use_depth:\n",
    "                object_mask = dig_outputs['accumulation']>.9\n",
    "                if metric_depth:\n",
    "                    physical_depth = dig_outputs['depth']/pipeline.datamanager.train_dataset._dataparser_outputs.dataparser_scale\n",
    "                    valids = object_mask & (~self.frame_depth.isnan())\n",
    "                    pix_loss = (physical_depth - self.frame_depth)[valids]**2\n",
    "                    pix_loss = torch.where(pix_loss<pix_loss.quantile(.7),pix_loss,torch.tensor(0,dtype=torch.float32,device='cuda'))\n",
    "                    loss = loss + pix_loss.mean()\n",
    "                else:\n",
    "                    # This is ranking loss for monodepth (which is disparity)\n",
    "                    disparity = 1.0 / dig_outputs['depth']\n",
    "                    N = 20000\n",
    "                    valid_ids = torch.where(object_mask)\n",
    "                    rand_samples = torch.randint(0,valid_ids[0].shape[0],(N,),device='cuda')\n",
    "                    rand_samples = (valid_ids[0][rand_samples],valid_ids[1][rand_samples])\n",
    "                    rend_samples = disparity[rand_samples]\n",
    "                    mono_samples = self.frame_depth[rand_samples]\n",
    "                    rank_loss = depth_ranking_loss(rend_samples,mono_samples)\n",
    "                    loss = loss + .5*rank_loss\n",
    "            if use_rgb:\n",
    "                loss = loss + .05*(dig_outputs['rgb']-self.rgb_frame).abs().mean()\n",
    "            if self.use_atap:\n",
    "                null_weights = torch.ones_like(self.connectivity_weights)\n",
    "                weights = torch.clip(null_weights,0,1)\n",
    "                atap_loss = self.atap(weights)\n",
    "                rigidity_loss = .02*(1-weights).mean()\n",
    "                symmetric_loss = (weights - weights.T).abs().mean()\n",
    "                #maximize the connectivity weights, as well as similarity\n",
    "                loss = loss + atap_loss + symmetric_loss + rigidity_loss\n",
    "            loss.backward()\n",
    "            if not accumulate_grad:\n",
    "                self.optimizer.step()\n",
    "                scheduler.step()\n",
    "        #reset lr\n",
    "        self.optimizer.param_groups[0]['lr'] = self.pose_lr\n",
    "        return dig_outputs\n",
    "    \n",
    "    def apply_to_model(self,pose_deltas):\n",
    "        \"\"\"\n",
    "        Takes the current pose_deltas and applies them to each of the group masks\n",
    "        \"\"\"\n",
    "        from viser.transforms import SO3\n",
    "\n",
    "        self.reset_transforms()\n",
    "        quat_deltas = torch.empty((self.dig_model.num_points,4),dtype=torch.float32,device='cuda', requires_grad=False)\n",
    "        Hs = torch.empty((self.dig_model.num_points,3,4),dtype=torch.float32,device='cuda', requires_grad=True)\n",
    "        for i,mask in enumerate(self.group_masks):\n",
    "            ps = pose_deltas[i:i+1,3:]/pose_deltas[i:i+1,3:].norm(dim=1,keepdim=True)\n",
    "            quat_deltas = torch.where(mask[...,None],ps,quat_deltas)\n",
    "            H = torch.cat([normalized_quat_to_rotmat(ps),pose_deltas[i:i+1,:3].T.unsqueeze(0)],dim=2)\n",
    "            Hs = torch.where(mask[...,None,None],H,Hs)\n",
    "        with torch.no_grad():\n",
    "            self.dig_model.gauss_params['quats'] = quatmul(quat_deltas,self.dig_model.gauss_params['quats'])\n",
    "        self.dig_model.gauss_params['means'] = (Hs[:,:3,3] + torch.bmm(Hs[:,:3,:3],self.dig_model.gauss_params['means'][...,None] - self.centroids[...,None]).squeeze()) + self.centroids\n",
    "\n",
    "\n",
    "    def register_keyframe(self):\n",
    "        \"\"\"\n",
    "        Saves the current pose_deltas as a keyframe\n",
    "        \"\"\"\n",
    "        self.keyframes.append(self.pose_deltas.detach().clone())\n",
    "\n",
    "    def apply_keyframe(self,i):\n",
    "        \"\"\"\n",
    "        Applies the ith keyframe to the pose_deltas\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.apply_to_model(self.keyframes[i])\n",
    "\n",
    "    def reset_transforms(self):\n",
    "        with torch.no_grad():\n",
    "            self.dig_model.gauss_params['means'] = self.init_means.clone()\n",
    "            self.dig_model.gauss_params['quats'] = self.init_quats.clone()\n",
    "\n",
    "    def set_frame(self, rgb_frame: torch.Tensor, depth: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Sets the rgb_frame to optimize the pose for\n",
    "        rgb_frame: HxWxC tensor image\n",
    "        init_c2o: initial camera to object transform (given whatever coordinates the self.dig_model is in)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.rgb_frame = resize(rgb_frame.permute(2,0,1), (self.init_c2o.height,self.init_c2o.width)).permute(1,2,0)\n",
    "            self.frame_pca_feats = self.dino_loader.get_pca_feats(rgb_frame.permute(2,0,1).unsqueeze(0),keep_cuda=True).squeeze()\n",
    "            self.frame_pca_feats = resize(self.frame_pca_feats.permute(2,0,1), (self.init_c2o.height,self.init_c2o.width)).permute(1,2,0)\n",
    "            if self.use_depth:\n",
    "                if depth is None:\n",
    "                    depth = get_depth((self.rgb_frame*255).to(torch.uint8))\n",
    "                self.frame_depth = resize(depth.unsqueeze(0), (self.init_c2o.height,self.init_c2o.width)).squeeze().unsqueeze(-1)\n",
    "\n",
    "\n",
    "MATCH_RESOLUTION = 500\n",
    "camera_input = 'iphone' # ['train_cam', 'iphone','zed', 'iphone_vertical','zed_svo']\n",
    "video_path = Path(\"motion_vids/nerfgun_interact.MOV\")\n",
    "svo_path = Path(\"motion_vids/nerfgun_cock.svo2\")\n",
    "start_time = 0.3\n",
    "\n",
    "\n",
    "if camera_input == 'train_cam':\n",
    "    init_cam,data = pipeline.datamanager.next_train(0)\n",
    "    view_cam_pose = pipeline.viewer_control.get_camera(200,None,0)\n",
    "    init_cam.camera_to_worlds = view_cam_pose.camera_to_worlds\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input == 'iphone':\n",
    "    init_cam = Cameras(camera_to_worlds=pipeline.viewer_control.get_camera(200,None,0).camera_to_worlds,fx = 1140.9495344705715,fy = 1137.8697337462174,cx = 1280/2,cy = 720/2,width=1280,height=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input == 'iphone_vertical':\n",
    "    init_cam = Cameras(camera_to_worlds=pipeline.viewer_control.get_camera(200,None,0).camera_to_worlds,fy = 1140.9495344705715,fx = 1137.8697337462174,cy = 1280/2,cx = 720/2,height=1280,width=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input in ['zed','zed_svo']:\n",
    "    try:\n",
    "        zed.cam.close()\n",
    "        del zed\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        zed = Zed(recording_file=str(svo_path.absolute()) if camera_input == 'zed_svo' else None, start_time=start_time)\n",
    "    fps = 30\n",
    "    left_rgb,_,_ = zed.get_frame()\n",
    "    K = zed.get_K()\n",
    "    init_cam = Cameras(camera_to_worlds=pipeline.viewer_control.get_camera(200,None,0).camera_to_worlds,fx = K[0,0],fy = K[1,1],cx = K[0,2],cy = K[1,2],width=1920,height=1080)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "outputs = pipeline.model.get_outputs_for_camera(init_cam)\n",
    "if pipeline.cluster_labels is not None:\n",
    "    labels = pipeline.cluster_labels.int()\n",
    "    group_masks = [(cid == labels).cuda() for cid in range(labels.max() + 1)]\n",
    "else:\n",
    "    group_masks = [torch.ones(pipeline.model.num_points).bool().cuda()]\n",
    "optimizer = RigidGroupOptimizer(pipeline.model,dino_loader,init_cam,group_masks,render_lock = v.train_lock)\n",
    "rgb_renders = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if camera_input in ['zed','zed_svo']:\n",
    "    left_rgb, right_rgb,depth = zed.get_frame()\n",
    "    target_frame_rgb = (left_rgb/255)\n",
    "    right_frame_rgb = (right_rgb/255)\n",
    "    optimizer.set_frame(target_frame_rgb,depth=depth)\n",
    "else:\n",
    "    assert video_path.exists()\n",
    "    motion_clip = cv2.VideoCapture(str(video_path.absolute()))\n",
    "    start=0\n",
    "    end=8\n",
    "    fps = 30\n",
    "    frame = get_vid_frame(motion_clip,start)\n",
    "    target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "    optimizer.set_frame(target_frame_rgb)\n",
    "_,axs = plt.subplots(1,2,figsize=(10,4))\n",
    "axs[0].imshow(outputs[\"rgb\"].detach().cpu().numpy())\n",
    "axs[1].imshow(target_frame_rgb.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.utils.colormaps import apply_depth_colormap\n",
    "import tqdm\n",
    "import moviepy.editor as mpy\n",
    "import plotly.express as px\n",
    "def plotly_render(frame):\n",
    "    fig = px.imshow(frame)\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=0, b=0),showlegend=False,yaxis_visible=False, yaxis_showticklabels=False,xaxis_visible=False, xaxis_showticklabels=False\n",
    "    )\n",
    "    return fig\n",
    "fig = plotly_render(outputs['rgb'].detach().cpu().numpy())\n",
    "try:\n",
    "    frame_vis.remove()\n",
    "except:\n",
    "    pass\n",
    "frame_vis = pipeline.viewer_control.viser_server.add_gui_plotly(fig, 9/16)\n",
    "try:\n",
    "    animate_button.remove()\n",
    "    frame_slider.remove()\n",
    "    reset_button.remove()\n",
    "except:\n",
    "    pass\n",
    "def composite_vis_frame(target_frame_rgb,outputs):\n",
    "    target_vis_frame = resize(target_frame_rgb.permute(2,0,1),(outputs[\"rgb\"].shape[0],outputs[\"rgb\"].shape[1])).permute(1,2,0)\n",
    "    # composite the outputs['rgb'] on top of target_vis frame\n",
    "    target_vis_frame = target_vis_frame*0.5 + outputs[\"rgb\"]*0.5\n",
    "    return target_vis_frame\n",
    "if camera_input in ['zed','zed_svo']:\n",
    "    while True:\n",
    "        # If input camera is the zed, just loop it indefinitely until no more frames\n",
    "        left_rgb, _, depth = zed.get_frame()\n",
    "        if left_rgb is None:\n",
    "            break\n",
    "        target_frame_rgb = left_rgb/255\n",
    "        optimizer.set_frame(target_frame_rgb,depth=depth)\n",
    "        outputs = optimizer.step(50, use_depth=True, metric_depth=True)\n",
    "        v._trigger_rerender()\n",
    "        optimizer.register_keyframe()\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "elif camera_input in ['iphone','iphone_vertical','train_cam']:\n",
    "    # Otherwise procces the video\n",
    "    if len(rgb_renders)==0:\n",
    "        for i in tqdm.tqdm(range(10)):\n",
    "            target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "            vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "            fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "            frame_vis.figure = fig\n",
    "            rgb_renders.append(vis_frame*255)\n",
    "            outputs = optimizer.step(10, use_depth=i>5, metric_depth=False)\n",
    "\n",
    "    for t in tqdm.tqdm(np.linspace(start,end,int((end-start)*fps))):\n",
    "        frame = get_vid_frame(motion_clip,t)\n",
    "        target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "        optimizer.set_frame(target_frame_rgb)\n",
    "        outputs = optimizer.step(50, metric_depth=False)\n",
    "        optimizer.register_keyframe()\n",
    "        v._trigger_rerender()\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "#save as an mp4\n",
    "out_clip = mpy.ImageSequenceClip(rgb_renders, fps=fps)  \n",
    "\n",
    "output_folder = Path(\"renders/nerfgun\")\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "fname = str(output_folder / \"optimizer_out_sched_005_0005_50.mp4\")\n",
    "\n",
    "out_clip.write_videofile(fname, fps=fps,codec='libx264')\n",
    "out_clip.write_videofile(fname.replace('.mp4','_mac.mp4'),fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "\n",
    "# Populate some viewer elements to visualize the animation\n",
    "animate_button = v.viser_server.add_gui_button(\"Play Animation\")\n",
    "frame_slider = v.viser_server.add_gui_slider(\"Frame\",0,len(optimizer.keyframes)-1,1,0)\n",
    "reset_button = v.viser_server.add_gui_button(\"Reset Transforms\")\n",
    "\n",
    "@animate_button.on_click\n",
    "def play_animation(_):\n",
    "    for i in range(len(optimizer.keyframes)):\n",
    "        optimizer.apply_keyframe(i)\n",
    "        v._trigger_rerender()\n",
    "        time.sleep(1/fps)\n",
    "@frame_slider.on_update\n",
    "def apply_keyframe(_):\n",
    "    optimizer.apply_keyframe(frame_slider.value)\n",
    "    v._trigger_rerender()\n",
    "@reset_button.on_click\n",
    "def reset_transforms(_):\n",
    "    optimizer.reset_transforms()\n",
    "    v._trigger_rerender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    render_button.remove()\n",
    "    filename_input.remove()\n",
    "    status_mkdown.remove()\n",
    "except:\n",
    "    pass\n",
    "import viser\n",
    "filename_input = v.viser_server.add_gui_text(\"File Name\",\"render\")\n",
    "status_mkdown = v.viser_server.add_gui_markdown(\" \")\n",
    "render_button = v.viser_server.add_gui_button(\"Render Animation\",color='green',icon=viser.Icon.MOVIE)\n",
    "@render_button.on_click\n",
    "def render(_):\n",
    "    render_button.disabled = True\n",
    "    render_frames = []\n",
    "    camera = pipeline.viewer_control.get_camera(1080,1920,0)\n",
    "    for i in tqdm.tqdm(range(len(optimizer.keyframes))):\n",
    "        status_mkdown.content = f\"Rendering...{i/len(optimizer.keyframes)}\"\n",
    "        pipeline.model.eval()\n",
    "        optimizer.apply_keyframe(i)\n",
    "        with torch.no_grad():\n",
    "            outputs = pipeline.model.get_outputs_for_camera(camera)\n",
    "        render_frames.append(outputs[\"rgb\"].detach().cpu().numpy()*255)\n",
    "    status_mkdown.content = \"Saving...\"\n",
    "    out_clip = mpy.ImageSequenceClip(render_frames, fps=fps)\n",
    "    fname = {filename_input.value}\n",
    "    out_clip.write_videofile(f\"renders/{fname}.mp4\", fps=fps,codec='libx264')\n",
    "    out_clip.write_videofile(f\"renders/{fname}_mac.mp4\", fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "    v.viser_server.send_file_download(f\"{fname}_mac.mp4\",open(f\"renders/{fname}_mac.mp4\",'rb').read())\n",
    "    status_mkdown.content = \"Done!\"\n",
    "    render_button.disabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "please",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
